{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO optimizations for Knowledge graphs \n",
    "\n",
    "The goal of this notebook is to showcase performance optimizations for the ConvE knowledge graph embeddings model using the Intel® Distribution of OpenVINO™ Toolkit.\n",
    "The optimizations process contains the following steps:\n",
    "1. Train the ConvE model using the countries dataset\n",
    "2. Export the trained model to OpenVINO intermediate representation (IR)\n",
    "3. Compare the inference performance with the optimized OpenVINO model\n",
    "\n",
    "The ConvE model we use is an implementation of the paper Convolutional 2D Knowledge Graph Embeddings (https://arxiv.org/abs/1707.01476). The sample dataset was downloaded from: https://github.com/TimDettmers/ConvE/tree/master/countries/countries_S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Windows, add the directory that contains cl.exe to the PATH to enable PyTorch to find the\n",
    "# required C++ tools. This code assumes that Visual Studio 2019 is installed in the default\n",
    "# directory. If you have a different C++ compiler, please add the correct path to os.environ[\"PATH\"]\n",
    "# directly. Note that the C++ Redistributable is not enough to run this notebook.\n",
    "\n",
    "# Adding the path to os.environ[\"LIB\"] is not always required - it depends on the system's configuration\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    import distutils.command.build_ext\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    VS_INSTALL_DIR = r\"C:/Program Files (x86)/Microsoft Visual Studio\"\n",
    "    cl_paths = sorted(list(Path(VS_INSTALL_DIR).glob(\"**/Hostx86/x64/cl.exe\")))\n",
    "    if len(cl_paths) == 0:\n",
    "        raise ValueError(\n",
    "            \"Cannot find Visual Studio. This notebook requires a C++ compiler. If you installed \"\n",
    "            \"a C++ compiler, please add the directory that contains cl.exe to `os.environ['PATH']`.\"\n",
    "        )\n",
    "    else:\n",
    "        # If multiple versions of MSVC are installed, get the most recent version\n",
    "        cl_path = cl_paths[-1]\n",
    "        vs_dir = str(cl_path.parent)\n",
    "        os.environ[\"PATH\"] += f\"{os.pathsep}{vs_dir}\"\n",
    "        # Code for finding the library dirs from\n",
    "        # https://stackoverflow.com/questions/47423246/get-pythons-lib-path\n",
    "        d = distutils.core.Distribution()\n",
    "        b = distutils.command.build_ext.build_ext(d)\n",
    "        b.finalize_options()\n",
    "        os.environ[\"LIB\"] = os.pathsep.join(b.library_dirs)\n",
    "        print(f\"Added {vs_dir} to PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.nn import functional as F, Parameter\n",
    "from torch.nn.init import xavier_normal_\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings: Including path to the serialized model files, input data files as well as hyperparameter settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "modelpath = Path('models/conve.pt')\n",
    "\n",
    "entdatapath = Path('data/countries_S1/kg_training_entids.txt')            # Path to the file containing the entities and entity IDs\n",
    "reldatapath = Path('data/countries_S1/kg_training_relids.txt')            # Path to the file containing the relations and relation IDs\n",
    "traindatapath = Path('data/countries_S1/e1rel_to_e2_train.json')          # Path to the training data file\n",
    "testdatapath = Path('data/countries_S1/e1rel_to_e2_ranking_test.json')    # Path to the test data file\n",
    "\n",
    "batch_size = 1                                                      # Batch size\n",
    "input_dropout = 0.2                                                 # Input dropout for the model\n",
    "dropout = 0.3                                                       # Dropout for the model\n",
    "feature_map_dropout = 0.2                                           # Feature map dropout for the model\n",
    "use_bias = True \n",
    "emb_dim = 300                                                       # Entity and relation embedding dimensions\n",
    "lr = 0.001                                                          # Learning rate\n",
    "l2 = 5e-4                                                           # L2 regularization\n",
    "label_smoothing_epsilon = 0.1                                       # Label smoothing epsilon\n",
    "max_epochs = 10                                                     # Max epochs\n",
    "\n",
    "top_k = 2                                                          # Top K vals to consider from the predictions\n",
    "\n",
    "\n",
    "### Required for OpenVINO conversion\n",
    "output_dir = Path(\"models\")\n",
    "base_model_name = \"conve\"\n",
    "\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
    "fp32_onnx_path = Path(output_dir / (base_model_name + \"_fp32\")).with_suffix(\".onnx\")\n",
    "fp32_ir_path = fp32_onnx_path.with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the ConvE model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvE(torch.nn.Module):\n",
    "    def __init__(self, num_entities, num_relations):\n",
    "        super(ConvE, self).__init__()\n",
    "        # Embedding tables for entity and relations with num_uniq_ent in y-dim, emb_dim in x-dim\n",
    "        self.emb_e = torch.nn.Embedding(num_entities, emb_dim, padding_idx=0)\n",
    "        self.ent_weights_matrix = torch.ones([num_entities, emb_dim], dtype=torch.float64)\n",
    "        self.emb_rel = torch.nn.Embedding(num_relations, emb_dim, padding_idx=0)\n",
    "        self.ne = num_entities\n",
    "        self.nr = num_relations\n",
    "        self.inp_drop = torch.nn.Dropout(input_dropout)\n",
    "        self.hidden_drop = torch.nn.Dropout(dropout)\n",
    "        self.feature_map_drop = torch.nn.Dropout2d(feature_map_dropout)\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, (3, 3), 1, 0, bias=use_bias)\n",
    "        self.bn0 = torch.nn.BatchNorm2d(1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(32)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(emb_dim)\n",
    "        self.ln0 = torch.nn.LayerNorm(emb_dim)\n",
    "        self.register_parameter('b', Parameter(torch.zeros(num_entities)))\n",
    "        self.fc = torch.nn.Linear(16128,emb_dim)\n",
    "    \n",
    "    def init(self):\n",
    "        # Xavier initialization\n",
    "        xavier_normal_(self.emb_e.weight.data)\n",
    "        xavier_normal_(self.emb_rel.weight.data)\n",
    "        \n",
    "    \n",
    "    def forward(self, e1, rel, print_pred=False):\n",
    "        batch_size = 1\n",
    "        e1_embedded= self.emb_e(e1).view(-1, 1, 10, 30)\n",
    "        rel_embedded = self.emb_rel(rel).view(-1, 1, 10, 30)\n",
    "        stacked_inputs = torch.cat([e1_embedded, rel_embedded], 2)\n",
    "\n",
    "        stacked_inputs = self.bn0(stacked_inputs)\n",
    "        x= self.inp_drop(stacked_inputs)\n",
    "        x= self.conv1(x)\n",
    "        x= self.bn1(x)\n",
    "        x= F.relu(x)\n",
    "        x = self.feature_map_drop(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.hidden_drop(x)\n",
    "        # Try Layer norm instead of batch norm\n",
    "        #x = self.bn2(x)\n",
    "        x = self.ln0(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.mm(x, self.emb_e.weight.transpose(1,0)) # shape (batch, n_ent)\n",
    "        x = self.hidden_drop(x)\n",
    "        x += self.b.expand_as(x)\n",
    "        pred = torch.nn.functional.softmax(x, dim=1)\n",
    "        #print(pred.shape)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        super(DataLoader, self).__init__()\n",
    "        \n",
    "        self.ent_path = entdatapath\n",
    "        self.rel_path = reldatapath\n",
    "        self.train_file = traindatapath\n",
    "        self.test_file = testdatapath\n",
    "        self.entity_ids = self.load_data(self.ent_path) \n",
    "        self.ids2entities =  self.id2ent(self.ent_path) \n",
    "        self.rel_ids =  self.load_data(self.rel_path)\n",
    "        self.ids2rel =  self.id2rel(self.rel_path) \n",
    "        self.train_triples_list = self.convert_triples(self.train_file)\n",
    "        self.test_triples_list = self.convert_triples(self.test_file)\n",
    "\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        item_dict = {}\n",
    "        with open(data_path) as df:\n",
    "            lines = df.readlines()\n",
    "            for line in lines:\n",
    "                name, id = line.strip().split('\\t')\n",
    "                item_dict[name] = int(id)\n",
    "        return item_dict\n",
    "    \n",
    "    def id2ent(self, data_path):\n",
    "        item_dict = {}\n",
    "        with open(data_path) as df:\n",
    "            lines = df.readlines()\n",
    "            for line in lines:\n",
    "                name, id = line.strip().split('\\t')\n",
    "                item_dict[int(id)] = name\n",
    "        return item_dict\n",
    "    \n",
    "    def id2rel(self, data_path):\n",
    "        item_dict = {}\n",
    "        with open(data_path) as df:\n",
    "            lines = df.readlines()\n",
    "            for line in lines:\n",
    "                name, id = line.strip().split('\\t')\n",
    "                item_dict[int(id)] = name\n",
    "        return item_dict\n",
    "    \n",
    "    def convert_triples(self, data_path):\n",
    "        triples_list = []\n",
    "        with open(data_path) as df:\n",
    "            lines = df.readlines()\n",
    "            for line in lines:\n",
    "                item_dict = json.loads(line.strip())\n",
    "                h = item_dict['e1']\n",
    "                r = item_dict['rel']\n",
    "                t = item_dict['e2_multi1'].split('\\t')\n",
    "                hrt_list = []\n",
    "                hrt_list.append(self.entity_ids[h])\n",
    "                hrt_list.append(self.rel_ids[r])\n",
    "                t_ents = []\n",
    "                for t_idx in t:\n",
    "                    t_ents.append(self.entity_ids[t_idx])\n",
    "                hrt_list.append(t_ents)\n",
    "                triples_list.append(hrt_list)\n",
    "        return triples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the ConvE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]<ipython-input-6-62475bfa34d2>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = model.loss(logits, torch.tensor(e2_multi))\n",
      "100%|██████████| 10/10 [02:12<00:00, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training the ConvE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = DataLoader()\n",
    "num_entities =  len(data.entity_ids)\n",
    "num_relations =  len(data.rel_ids)\n",
    "triples_list = data.train_triples_list\n",
    "num_train_samples = len(triples_list)\n",
    "num_train_steps = math.ceil(num_train_samples/int(batch_size))\n",
    "\n",
    "model = ConvE(num_entities, num_relations)\n",
    "model.init()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Fitting the Model\n",
    "model.train()\n",
    "loss_values = []\n",
    "for ep in tqdm(range(max_epochs)):\n",
    "    epoch_loss = 0\n",
    "    # TODO: Check support for higher batch size > 1 training\n",
    "    for counter in range(num_train_steps):\n",
    "        optimizer.zero_grad()\n",
    "        e2_multi = torch.zeros(batch_size, num_entities)\n",
    "        train_sample = triples_list[counter:counter+int(batch_size)]\n",
    "        h,r,t = train_sample[0]\n",
    "        logits = model.forward(torch.tensor(h), torch.tensor(r), print_pred=False)\n",
    "        for t_id in t:\n",
    "            e2_multi[0][t_id] = 1.0\n",
    "        loss = model.loss(logits, torch.tensor(e2_multi))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = torch.sum(loss)\n",
    "        #print('Batch {}: Batch loss:{}'.format(counter, batch_loss))\n",
    "        epoch_loss += batch_loss\n",
    "        loss_values.append(epoch_loss)\n",
    "        counter = counter + int(batch_size)\n",
    "    #print('Epoch loss:{}'.format(epoch_loss))\n",
    "print(f'Finished training the ConvE model')\n",
    "plt.plot(np.array(loss_values))\n",
    "\n",
    "\n",
    "# Save model    \n",
    "torch.save(model.state_dict(), modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained ConvE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Entity:zambia, Relation:locatedin, Model Predictions:eastern_africa, Ground truth Target:africa\n",
      "Entity:zambia, Relation:locatedin, Model Predictions:africa, Ground truth Target:africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:morocco, Relation:locatedin, Model Predictions:northern_africa, Ground truth Target:africa\n",
      "Entity:morocco, Relation:locatedin, Model Predictions:europe, Ground truth Target:africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:san_marino, Relation:locatedin, Model Predictions:southern_europe, Ground truth Target:europe\n",
      "Entity:san_marino, Relation:locatedin, Model Predictions:europe, Ground truth Target:europe\n",
      "--------------------------------------------------------------------------\n",
      "Entity:canada, Relation:locatedin, Model Predictions:northern_america, Ground truth Target:americas\n",
      "Entity:canada, Relation:locatedin, Model Predictions:americas, Ground truth Target:americas\n",
      "--------------------------------------------------------------------------\n",
      "Entity:uganda, Relation:locatedin, Model Predictions:eastern_africa, Ground truth Target:africa\n",
      "Entity:uganda, Relation:locatedin, Model Predictions:africa, Ground truth Target:africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:belgium, Relation:locatedin, Model Predictions:western_europe, Ground truth Target:western_europe\n",
      "Entity:belgium, Relation:locatedin, Model Predictions:europe, Ground truth Target:western_europe\n",
      "--------------------------------------------------------------------------\n",
      "Entity:syria, Relation:locatedin, Model Predictions:western_asia, Ground truth Target:asia\n",
      "Entity:syria, Relation:locatedin, Model Predictions:asia, Ground truth Target:asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:mali, Relation:locatedin, Model Predictions:western_africa, Ground truth Target:africa\n",
      "Entity:mali, Relation:locatedin, Model Predictions:africa, Ground truth Target:africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:russia, Relation:locatedin, Model Predictions:europe, Ground truth Target:europe\n",
      "Entity:russia, Relation:locatedin, Model Predictions:eastern_europe, Ground truth Target:europe\n",
      "--------------------------------------------------------------------------\n",
      "Entity:israel, Relation:locatedin, Model Predictions:western_asia, Ground truth Target:asia\n",
      "Entity:israel, Relation:locatedin, Model Predictions:asia, Ground truth Target:asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:republic_of_the_congo, Relation:locatedin, Model Predictions:middle_africa, Ground truth Target:middle_africa\n",
      "Entity:republic_of_the_congo, Relation:locatedin, Model Predictions:africa, Ground truth Target:middle_africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:guatemala, Relation:locatedin, Model Predictions:central_america, Ground truth Target:americas\n",
      "Entity:guatemala, Relation:locatedin, Model Predictions:americas, Ground truth Target:americas\n",
      "--------------------------------------------------------------------------\n",
      "Entity:sri_lanka, Relation:locatedin, Model Predictions:southern_asia, Ground truth Target:southern_asia\n",
      "Entity:sri_lanka, Relation:locatedin, Model Predictions:asia, Ground truth Target:southern_asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:monaco, Relation:locatedin, Model Predictions:western_europe, Ground truth Target:western_europe\n",
      "Entity:monaco, Relation:locatedin, Model Predictions:europe, Ground truth Target:western_europe\n",
      "--------------------------------------------------------------------------\n",
      "Entity:croatia, Relation:locatedin, Model Predictions:southern_europe, Ground truth Target:europe\n",
      "Entity:croatia, Relation:locatedin, Model Predictions:europe, Ground truth Target:europe\n",
      "--------------------------------------------------------------------------\n",
      "Entity:paraguay, Relation:locatedin, Model Predictions:south_america, Ground truth Target:south_america\n",
      "Entity:paraguay, Relation:locatedin, Model Predictions:americas, Ground truth Target:south_america\n",
      "--------------------------------------------------------------------------\n",
      "Entity:poland, Relation:locatedin, Model Predictions:europe, Ground truth Target:europe\n",
      "Entity:poland, Relation:locatedin, Model Predictions:eastern_europe, Ground truth Target:europe\n",
      "--------------------------------------------------------------------------\n",
      "Entity:hong_kong, Relation:locatedin, Model Predictions:eastern_asia, Ground truth Target:eastern_asia\n",
      "Entity:hong_kong, Relation:locatedin, Model Predictions:china, Ground truth Target:eastern_asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:libya, Relation:locatedin, Model Predictions:northern_africa, Ground truth Target:africa\n",
      "Entity:libya, Relation:locatedin, Model Predictions:africa, Ground truth Target:africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:jordan, Relation:locatedin, Model Predictions:western_asia, Ground truth Target:asia\n",
      "Entity:jordan, Relation:locatedin, Model Predictions:asia, Ground truth Target:asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:yemen, Relation:locatedin, Model Predictions:western_asia, Ground truth Target:asia\n",
      "Entity:yemen, Relation:locatedin, Model Predictions:asia, Ground truth Target:asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:saudi_arabia, Relation:locatedin, Model Predictions:western_asia, Ground truth Target:asia\n",
      "Entity:saudi_arabia, Relation:locatedin, Model Predictions:asia, Ground truth Target:asia\n",
      "--------------------------------------------------------------------------\n",
      "Entity:mauritania, Relation:locatedin, Model Predictions:western_africa, Ground truth Target:africa\n",
      "Entity:mauritania, Relation:locatedin, Model Predictions:northern_africa, Ground truth Target:africa\n",
      "--------------------------------------------------------------------------\n",
      "Entity:suriname, Relation:locatedin, Model Predictions:south_america, Ground truth Target:south_america\n",
      "Entity:suriname, Relation:locatedin, Model Predictions:americas, Ground truth Target:south_america\n",
      "--------------------------------------------------------------------------\n",
      "Average time taken for inference: 2.3411711057027182 ms\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader()\n",
    "num_entities =  len(data.entity_ids)\n",
    "num_relations =  len(data.rel_ids)\n",
    "\n",
    "names_dict = data.ids2entities\n",
    "rel_dict = data.ids2rel\n",
    "\n",
    "model = ConvE(num_entities, num_relations)\n",
    "model.load_state_dict(torch.load(modelpath))\n",
    "model.eval()\n",
    "\n",
    "inf_times = []\n",
    "\n",
    "triples_list = data.test_triples_list\n",
    "num_test_samples = len(triples_list)\n",
    "for i in range(num_test_samples):\n",
    "    test_sample = triples_list[i]\n",
    "    #print('Testing sample:{}'.format(test_sample))\n",
    "    h,r,t = test_sample\n",
    "    start_time = time.time()\n",
    "    logits = model.forward(torch.tensor(h), torch.tensor(r), print_pred=False)\n",
    "    end_time = time.time()\n",
    "    inf_times.append(end_time-start_time)\n",
    "    score, pred = torch.topk(logits, top_k, 1)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    for j, id in enumerate(pred[0].cpu().detach().numpy()): \n",
    "        pred_entity = names_dict[id]\n",
    "        print('Entity:{}, Relation:{}, Model Predictions:{}, Ground truth Target:{}'.format(names_dict[h], rel_dict[r], names_dict[id], names_dict[t[0]]))\n",
    "\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "avg_pt_time = np.mean(inf_times)*1000\n",
    "print(f'Average time taken for inference: {avg_pt_time} ms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the trained PyTorch model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the trained conve model to ONNX format\n"
     ]
    }
   ],
   "source": [
    "print(f'Converting the trained conve model to ONNX format')\n",
    "torch.onnx.export(model, (torch.tensor(1), torch.tensor(1)), fp32_onnx_path, verbose=False, opset_version=11, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the ONNX model to OpenVINO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fp32_ir_path.exists():\n",
    "    !mo --input_model \"$fp32_onnx_path\" --input='input.1[1],input.2[1]' --data_type FP32 --output_dir \"$output_dir\"\n",
    "    assert fp32_ir_path.exists(), \"The IR of FP32 model wasn't created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the converted OpenVINO model using benchmark app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark FP32 model (IR)\n",
      "Count:          448356 iterations\n",
      "Duration:       15000.52 ms\n",
      "Latency:\n",
      "Throughput: 29889.35 FPS\n",
      "Avg OV latency: 0.35 ms\n",
      "Speedup with OV optimizations: 6.69 X\n"
     ]
    }
   ],
   "source": [
    "def parse_benchmark_output(benchmark_output: str):\n",
    "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
    "    parsed_output = [line for line in benchmark_output if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "    avg_latency_str = [line for line in benchmark_output if (\"AVG\" in line)]\n",
    "    avg_ov_latency = float(avg_latency_str[0].strip().split('AVG')[1].split('ms')[0].split(':')[1].strip())\n",
    "    print(f'Avg OV latency: {avg_ov_latency} ms')\n",
    "    return avg_ov_latency\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model (IR)')\n",
    "benchmark_output = ! benchmark_app -m \"$fp32_ir_path\" -d CPU -api async -t 15 -shape 'input.1[1],input.2[1]'\n",
    "avg_ov_latency = parse_benchmark_output(benchmark_output)\n",
    "print(f'Speedup with OV optimizations: {round(float(avg_pt_time)/float(avg_ov_latency),2)} X')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e339aa45fb9f0301d2f56c13d53088845d2fe9489fd39416395d98a5dd08590"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('hftests': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
